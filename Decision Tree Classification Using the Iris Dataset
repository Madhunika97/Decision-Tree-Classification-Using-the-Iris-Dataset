                                                                     Decision Tree Classification Using the Iris Dataset

Introduction

In this project, we will explore the Decision Tree classification algorithm using the popular Iris dataset. This dataset consists of 150 samples of iris flowers, divided into three species: Setosa, Versicolor, and Virginica. Each sample contains four features: sepal length, sepal width, petal length, and petal width. The goal is to build a machine learning model that can accurately classify the species of iris flowers based on these features.

Libraries Used

To accomplish this project, we will use the following libraries:
- **Pandas**: For data manipulation and analysis.
- **NumPy**: For numerical operations.
- **Seaborn** and **Matplotlib**: For data visualization.
- **Scikit-learn**: For building and evaluating the machine learning model.

Data Loading

We start by loading the Iris dataset using the `load_iris` function from Scikit-learn.

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline 

from sklearn.datasets import load_iris
iris = load_iris()
iris.data
```

Building the Decision Tree Classifier

Next, we initialize a Decision Tree classifier. We will use the "entropy" criterion for the information gain calculation, which helps in making better splits.

```python
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion="entropy", random_state=35)

classifier.fit(iris.data, iris.target)
```

Visualizing the Decision Tree

To better understand how the classifier is making decisions, we visualize the tree using the `plot_tree` function. 

```python
tree.plot_tree(classifier, filled=True, feature_names=iris.feature_names, class_names=iris.target_names, fontsize=5)
plt.figure(figsize=(35, 20))
plt.show()
```

The visualization helps in understanding the feature importance and the decision-making process of the tree.

Evaluation of the Model

Train-Test Split

To evaluate the model, we split the dataset into training and testing sets. We will use 70% of the data for training and 30% for testing.

```python
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=35)

classifier.fit(X_train, y_train)
```

Predictions and Accuracy

After training the model, we make predictions on the test set and calculate the accuracy of the model.

```python
y_pred = classifier.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")
```

Additionally, we provide a classification report and a confusion matrix to better understand the model's performance:

```python
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
```

Feature Importance

Understanding which features contribute most to the model's decisions is crucial. We plot the feature importances using a bar plot.

```python
feature_importances = classifier.feature_importances_

plt.figure(figsize=(10, 5))
sns.barplot(x=iris.feature_names, y=feature_importances)
plt.title('Feature Importance')
plt.show()
```

Cross-Validation

To ensure our model's robustness, we use cross-validation. This involves splitting the dataset into multiple subsets and training the model multiple times.

```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(classifier, iris.data, iris.target, cv=5)
print(f"Cross-Validation Accuracy: {np.mean(scores) * 100:.2f}%")
```

Hyperparameter Tuning

Finally, we use Grid Search to find the best hyperparameters for our Decision Tree model. This helps in optimizing the model for better accuracy.

```python
from sklearn.model_selection import GridSearchCV
param_grid = {
    'max_depth': [3, 5, 7, None],
    'min_samples_split': [2, 4, 6],
    'min_samples_leaf': [1, 2, 3],
    'criterion': ['gini', 'entropy']
}

grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring='accuracy')
grid_search.fit(iris.data, iris.target)

print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best Cross-Validation Score: {grid_search.best_score_ * 100:.2f}%")
```

Conclusion

After completing this project, we can summarize the following findings:

1. **Model Performance**: The Decision Tree classifier demonstrated good accuracy in classifying the iris species, achieving over 90% accuracy in our evaluation.

2. **Feature Importance**: The analysis showed that petal length and petal width were the most significant features for classification.

3. **Cross-Validation**: The model's performance was consistent across different subsets of the data, indicating its robustness.

4. **Hyperparameter Optimization**: By using Grid Search, we identified the best hyperparameters, which could further enhance the model's performance.

This project provided an excellent opportunity to apply machine learning concepts, including data visualization, model evaluation, and hyperparameter tuning, and helped solidify the understanding of Decision Trees in a practical context.
